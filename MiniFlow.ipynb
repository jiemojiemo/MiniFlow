{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MiniFlow\n",
    "\n",
    "在学习TensorFlow之前，让我们学习可微分图（Differentiable graphs），这是TensorFlow用于运行和训练网络的基本抽象。我们将构建一个叫MiniFlow小型库，在这个过程中，我们将逐渐理解可微分图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph\n",
    "\n",
    "一个神经网络，可以看成是一张图，这张图由数学函数组成，比如线性组合，激活函数之类的。图中包含 **点(nodes)** 和 **边(edges)** 。\n",
    "\n",
    "点，可以看成是一个数学函数，利用上一层的输出作为输入。例如一个点可以表示为$f(x,y) = x+y$，$x,y$为上一层的输出，并作为该点的输入。\n",
    "\n",
    "边，是点与点之间的连接，使得值能在图中传播。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MiniFlow Architecture\n",
    "\n",
    "让我们开始构建MiniFLow。用`Node`表示一个点，`Node`接受一组输入，并计算出一个值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    def __init__(self, inbound_nodes=[]):\n",
    "        # 保存输入\n",
    "        self.inbound_nodes = inbound_nodes\n",
    "        # 定义输出\n",
    "        self.outbound_nodes = []\n",
    "        # 对于每一个输入，其输出就是这个点(有点绕，看代码)，进行点与点之间的连接\n",
    "        for n in self.inbound_nodes:\n",
    "            n.outbound_nodes.append(self)\n",
    "            \n",
    "        # 计算出来的值\n",
    "        self.value = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于每一个Node，它都应该能够向前(forward)或者向后(backward)传播。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    def __init__(self, inbound_nodes=[]):\n",
    "        # 保存输入\n",
    "        self.inbound_nodes = inbound_nodes\n",
    "        # 定义输出\n",
    "        self.outbound_nodes = []\n",
    "        # 对于每一个输入，其输出就是这个点(有点绕，看代码)，进行点与点之间的连接\n",
    "        for n in self.inbound_nodes:\n",
    "            n.outbound_nodes.append(self)\n",
    "            \n",
    "        # 计算出来的值\n",
    "        self.value = None\n",
    "        \n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        向前传播\n",
    "        \n",
    "        利用inbound_nodes计算输出值\n",
    "        \"\"\"\n",
    "        raise NotImplemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Input Subclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Node定义了点的基本熟悉和操作，然而不同类别的节点具有的操作也是不同的。例如，我们来实现Node的一个子类，`Input`。\n",
    "\n",
    "与Node的其他子类不同，Input并没有计算任何东西，只是存放了一个value，这个value可以是数据的特征或者一个权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Input(Node):\n",
    "    def __init__(self):\n",
    "        # Input 节点没有输入节点（它就是输入节点）\n",
    "        # 因此输入是空\n",
    "        Node.__init__(self)\n",
    "        \n",
    "    # Input 节点是唯一一种不需要输入的节点，其他类型的节点在做forward的时候都需要用到上一层的输入\n",
    "    def forward(self, value=None):\n",
    "        if value is not None:\n",
    "            self.value = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Add Subclass\n",
    "\n",
    "Add 节点，计算多个数的和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Add(Node):\n",
    "    def __init__(self, *inputs):\n",
    "        Node.__init__(self, list(inputs))\n",
    "        \n",
    "    def forward(self):\n",
    "        self.value = 0\n",
    "        for x in self.inbound_nodes:\n",
    "            self.value += x.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward propagation\n",
    "\n",
    "好的，让我们来测试下我们目前所写的代码。在这之前，我们先说一说[拓扑排序(topological sort)](https://en.wikipedia.org/wiki/Topological_sorting)。\n",
    "\n",
    "为了定义我们的神经网络，我们就要定义图的运算顺序。考虑到节点与节点之间的相互依赖的关系，我们需要将图\"扁平\"，拓扑排序就是干这个的，如下图所示![topological.jpg](topological.jpg)。拓扑排序如何实现的，并不重要，我们只要知道它的作用就行了。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将用`topological_sort()`对图进行\"扁平\"化，它接受一个`feed_dict`，在python中用字典实现，接下来我们演示下如何使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Sort generic nodes in topological order using Kahn's Algorithm.\n",
    "\n",
    "    `feed_dict`: A dictionary where the key is a `Input` node and the value is the respective value feed to that node.\n",
    "\n",
    "    Returns a list of sorted nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    input_nodes = [n for n in feed_dict.keys()]\n",
    "\n",
    "    G = {}\n",
    "    nodes = [n for n in input_nodes]\n",
    "    while len(nodes) > 0:\n",
    "        n = nodes.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outbound_nodes:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            nodes.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_nodes)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.outbound_nodes:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An example of feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.Input at 0x21ac5c64dd8>,\n",
       " <__main__.Input at 0x21ac5c64a58>,\n",
       " <__main__.Add at 0x21ac5c649b0>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = Input(), Input()\n",
    "add = Add(x, y)\n",
    "feed_dict = {x:10, y:20}\n",
    "sorted_nodes = topological_sort(feed_dict)\n",
    "sorted_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sorted_nodes用到的另外一个函数是`forward_pass()`，这个函数实际的让网络“跑”了起来。\n",
    "\n",
    "sorted_nodes是经过拓扑排序之后的有序节点，output_node是在sorted_nodes中某个节点同时也是我们希望得到其输出的节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(sorted_nodes):\n",
    "    \n",
    "    for n in sorted_nodes:\n",
    "        n.forward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing Values Forward\n",
    "\n",
    "向前网络我们已经大致搭建好了，我们来进行测试吧。\n",
    "\n",
    "这里，我们定义了三个输入 x，y和z，并计算它们的和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None 10 5 15\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "x,y,z = Input(), Input(),Input()\n",
    "\n",
    "f = Add(x, y, z)\n",
    "\n",
    "feed_dict = {x:10, y:5, z:15}\n",
    "\n",
    "sorted_nodes = topological_sort(feed_dict)\n",
    "\n",
    "# 未进行forward_pass，这个网络没有“跑”，只有输入节点有值\n",
    "print(f.value, x.value, y.value, z.value)\n",
    "\n",
    "# 让网络向前运行，\n",
    "forward_pass(sorted_nodes)\n",
    "print(f.value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "类似的，我们现在随手就能写出一个`Mul`类，用于计算节点之间的乘积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mul(Node):\n",
    "    def __init__(self, *inputs):\n",
    "        Node.__init__(self, list(inputs))\n",
    "        \n",
    "    def forward(self):\n",
    "        self.value = 1\n",
    "        for n in self.inbound_nodes:\n",
    "            self.value *= n.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750\n"
     ]
    }
   ],
   "source": [
    "x,y,z = Input(), Input(),Input()\n",
    "\n",
    "f = Mul(x, y, z)\n",
    "\n",
    "feed_dict = {x:10, y:5, z:15}\n",
    "\n",
    "sorted_nodes = topological_sort(feed_dict)\n",
    "forward_pass(sorted_nodes)\n",
    "print(f.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Function\n",
    "\n",
    "接下来，我们将构建一个更为复杂也更有用的节点：Linear。\n",
    "\n",
    "在神经网络中线性方程写成这样 $y = \\sum w_i x_i + b$。$x_i$是输入，$w_i$是权重，$b$是偏移量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Linear(Node):\n",
    "    def __init__(self, inputs, weights, bias):\n",
    "        Node.__init__(self, [inputs, weights, bias])\n",
    "        \n",
    "    def forward(self):\n",
    "        self.value = 0\n",
    "        inputs = self.inbound_nodes[0].value\n",
    "        weights = self.inbound_nodes[1].value\n",
    "        bias = self.inbound_nodes[2].value\n",
    "        \n",
    "        self.value = np.dot(inputs, weights) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-9.  4.]\n",
      " [-9.  4.]]\n"
     ]
    }
   ],
   "source": [
    "X, W, b = Input(), Input(), Input()\n",
    "\n",
    "f = Linear(X, W, b)\n",
    "\n",
    "X_ = np.array([[-1., -2.], [-1, -2]])\n",
    "W_ = np.array([[2., -3], [2., -3]])\n",
    "b_ = np.array([-3., -5])\n",
    "\n",
    "feed_dict = {X: X_, W: W_, b: b_}\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "forward_pass(graph)\n",
    "\n",
    "\"\"\"\n",
    "Output should be:\n",
    "[[-9., 4.],\n",
    "[-9., 4.]]\n",
    "\"\"\"\n",
    "print(f.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Function\n",
    "\n",
    "一个sigmoid函数可以定义为$sigmoid(x) = \\frac{1}{1+e^{-x}}$，它的导数与自身相关：$\\sigma'(x)=\\sigma(x)(1 - \\sigma(x))$。\n",
    "接下来，我们实现Sigmoid节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Node):\n",
    "    def __init__(self, node):\n",
    "        Node.__init__(self, [node])\n",
    "        \n",
    "    def _sigmoid(self, x):\n",
    "        return 1./(1 + np.exp(-x))\n",
    "        \n",
    "    def forward(self):\n",
    "        input_value = self.inbound_nodes[0].value\n",
    "        self.value = self._sigmoid(input_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.23394576e-04   9.82013790e-01]\n",
      " [  1.23394576e-04   9.82013790e-01]]\n"
     ]
    }
   ],
   "source": [
    "X, W, b = Input(), Input(), Input()\n",
    "\n",
    "f = Linear(X, W, b)\n",
    "g = Sigmoid(f)\n",
    "\n",
    "X_ = np.array([[-1., -2.], [-1., -2.]]) # 2x2\n",
    "W_ = np.array([[2., -3.], [2., -3.]]) # 2x2\n",
    "b_ = np.array([[-3., -5.]])\n",
    "\n",
    "feed_dict = {X:X_, W:W_, b:b_}\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "forward_pass(graph)\n",
    "\"\"\"\n",
    "Output should be:\n",
    "[[  1.23394576e-04   9.82013790e-01]\n",
    " [  1.23394576e-04   9.82013790e-01]]\n",
    "\"\"\"\n",
    "print(g.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost\n",
    "\n",
    "网络训练发生在向后传播，训练的过程就是让Cost变得最小。\n",
    "\n",
    "对于一个Cost，比如MSE，定义如下：\n",
    "$$\n",
    "C(w, b) = \\frac{1}{m}\\sum_x \\Vert y(x) - a\\Vert\n",
    "$$\n",
    "$w$是所有权重，$b$是所有偏移量，$m$是样本总数，$a$是指整个网络的输出，$y(x)$是$x$的标签。\n",
    "所谓的学习，就是通过调整权重和偏移让Cost变小的过程。调整的过程需要计算 Cost对w以及b的偏导\n",
    "\n",
    "接下来，我们设计一个MSE节点，用于计算MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE(Node):\n",
    "    def __init__(self, y, a):\n",
    "        Node.__init__(self, [y, a])\n",
    "        \n",
    "    def forward(self):\n",
    "        self.value = 0\n",
    "        y = self.inbound_nodes[0].value.reshape(-1, 1)\n",
    "        a = self.inbound_nodes[1].value.reshape(-1, 1)\n",
    "        m = a.size\n",
    "        \n",
    "        diff = y - a\n",
    "        \n",
    "        self.value = np.mean(diff**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.4166666667\n"
     ]
    }
   ],
   "source": [
    "y, a = Input(), Input()\n",
    "cost = MSE(y, a)\n",
    "\n",
    "y_ = np.array([[1, 2, 3]])\n",
    "a_ = np.array([[4.5, 5, 10]])\n",
    "\n",
    "feed_dict = {y:y_, a:a_}\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "forward_pass(graph)\n",
    "print(cost.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward propagation\n",
    "\n",
    "向前传播大致已经写好了，那接下来我们考虑向后传播的实现。\n",
    "\n",
    "稍微了解过神经网络的同学会知道，向后传播本质就是一个求偏导的过程。举个例子，假设我们有下面这样的一个网络![forward_pass](forward_pass.png)\n",
    "\n",
    "这个网络用我们现在的框架来写的话，应该长这样\n",
    "```python \n",
    "X, y = Input(), Input()\n",
    "W1, b1 = Input(), Input()\n",
    "W2, b2 = Input(), Input()\n",
    "\n",
    "l1 = Linear(X, W1, b1)\n",
    "s = Sigmoid(l1)\n",
    "l2 = Linear(s, W2, b2)\n",
    "cost = MSE(l2, y)```\n",
    "\n",
    "这个网络向后传播的过程是这样的![backward_pass](backward_pass.png)\n",
    "\n",
    "更直观的，我们用数学公式表达，可以写成这样\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial w_2} = \\frac{\\partial C}{\\partial _2} \\frac{\\partial l_2}{\\partial w_2}\n",
    "$$\n",
    "这就是求导的链式法则。我们的代码实现上就是根据链式法则来完成的。\n",
    "\n",
    "至此我们可以做一个大致的分析，假设，有一个$L$层节点，那么在向前传播`forward()`中，这个节点接收$L-1$层的节点作为输入，也就是代码中的`inbound_nodes`，并且根据`inbound_nodes`的值，进行运算，最终得到一个`value`。因此，向前传播的时候，传播的是运算结果，\"值\"。\n",
    "\n",
    "那么在向后传播中，一个$L$层的节点，接收的应该是$L+1$层网络的节点，也就是$L$层节点的输出节点`outbound_nodes`作为输入，并根据`outbound_nodes`的梯度，进行运算，最终得到一个`gradient`。因此，向后传播的时候，传播的是\"梯度\"，准确的说，应该是目标函数对当前节点的梯度。\n",
    "\n",
    "基于这样的想法，对原有的代码进行添加`backward`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    def __init__(self, inbound_nodes=[]):\n",
    "        self.inbound_nodes = inbound_nodes\n",
    "        \n",
    "        self.value = None\n",
    "        \n",
    "        self.outbound_nodes = []\n",
    "        \n",
    "        self.gradients = []\n",
    "        \n",
    "        for node in inbound_nodes:\n",
    "            node.outbound_nodes.append(self)\n",
    "            \n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        每一个继承了这个类的子类，都应该实现这个方法\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        每一个继承了这个类的子类，都应该实现这个方法\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_and_backward(graph):\n",
    "    # 向前传播\n",
    "    for n in graph:\n",
    "        n.forward()\n",
    "        \n",
    "    # 向后传播\n",
    "    for n in graph[::-1]:\n",
    "        n.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，对之前所有继承了`Node`节点的子类进行改写。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MSE \n",
    "均方误差MSE，为了求导方便，我们用矩阵向量的形式重写MSE的表达式，下面所有变量都是向量或者矩阵\n",
    "$$\n",
    "Cost(W,b) = \\frac{1}{m}\\Vert y - a\\Vert_2^2\n",
    "$$\n",
    "如果你对矩阵向量求导有了解，那么可以计算得到：\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial y} = \\frac{2}{m}(y - a) \\\\\n",
    "\\frac{\\partial C}{\\partial a} = \\frac{2}{m}(a - y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE(Node):\n",
    "    def __init__(self, y, a):\n",
    "        Node.__init__(self, [y, a])\n",
    "        \n",
    "    def forward(self):\n",
    "        y = self.inbound_nodes[0].value.reshape(-1,1)\n",
    "        a = self.inbound_nodes[1].value.reshape(-1,1)\n",
    "        self.m = self.inbound_nodes[0].value.shape[0]\n",
    "        self.diff = y - a\n",
    "        self.value = np.mean(self.diff**2)\n",
    "        \n",
    "    def backward(self):\n",
    "        self.gradients = {n:np.zeros_like(n.value) for n in self.inbound_nodes}\n",
    "        \n",
    "        # MSE 对 y 求偏导（动手拿笔算一算）\n",
    "        self.gradients[self.inbound_nodes[0]] = (2 / self.m) * self.diff\n",
    "        # MSE 对 a 求偏导\n",
    "        self.gradients[self.inbound_nodes[1]] = (-2 / self.m) * self.diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Node):\n",
    "    def __init__(self, X, W, b):\n",
    "        Node.__init__(self, [X, W, b])\n",
    "        \n",
    "    def forward(self):\n",
    "        self.value = 0\n",
    "        \n",
    "        X = self.inbound_nodes[0].value\n",
    "        W = self.inbound_nodes[1].value\n",
    "        b = self.inbound_nodes[2].value\n",
    "        \n",
    "        self.value = np.dot(X, W) + b\n",
    "        \n",
    "    def backward(self):\n",
    "        self.gradients = { n:np.zeros_like(n.value) for n in self.inbound_nodes}\n",
    "        \n",
    "        for n in self.outbound_nodes:\n",
    "            # 获取上一层对改节点的偏导\n",
    "            grad_cost = n.gradients[self]\n",
    "            \n",
    "            # 对 X 求偏导\n",
    "            self.gradients[self.inbound_nodes[0]] += np.dot(grad_cost, self.inbound_nodes[1].value.T)\n",
    "            # 对 W 求偏导\n",
    "            self.gradients[self.inbound_nodes[1]] += np.dot(self.inbound_nodes[0].value.T, grad_cost)\n",
    "            # 对 b 求偏导(不明白)\n",
    "            self.gradients[self.inbound_nodes[2]] += np.sum(grad_cost, axis=0, keepdims=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sigmoid\n",
    "\n",
    "一个sigmoid函数可以定义为$sigmoid(x) = \\frac{1}{1+e^{-x}}$，它的导数与自身相关：$\\sigma'(x)=\\sigma(x)(1 - \\sigma(x))$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Node):\n",
    "    def __init__(self, node):\n",
    "        Node.__init__(self, [node])\n",
    "        \n",
    "    def _sigmoid(self, x):\n",
    "        return 1. / (1 + np.exp(-x))\n",
    "    \n",
    "    def forward(self):\n",
    "        self.value = 0\n",
    "        \n",
    "        x = self.inbound_nodes[0].value\n",
    "        self.value = self._sigmoid(x)\n",
    "        \n",
    "    def backward(self):\n",
    "        self.gradients = {n:np.zeros_like(n.value) for n in self.inbound_nodes}\n",
    "        \n",
    "        for n in self.outbound_nodes:\n",
    "            grad_cost = n.gradients[self]\n",
    "            \n",
    "            sigmoid_value = self.value\n",
    "            self.gradients[self.inbound_nodes[0]] = grad_cost * sigmoid_value * (1 - sigmoid_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Input(Node):\n",
    "    def __init__(self):\n",
    "        Node.__init__(self)\n",
    "        \n",
    "    def forward(self):\n",
    "        pass\n",
    "\n",
    "    def backward(self):\n",
    "        # Input节点没有输入，因此它的梯度应该是0\n",
    "        self.gradients = {self:0}\n",
    "        #Weights and bias may be inputs, so you need to sum\n",
    "        #the gradient from output gradients.\n",
    "        for n in self.outbound_nodes:\n",
    "            grad_cost = n.gradients[self]\n",
    "            self.gradients[self] += grad_cost*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Add(Node):\n",
    "    def __init__(self, *input):\n",
    "        Node.__init__(self, list(input))\n",
    "        \n",
    "    def forward(self):\n",
    "        self.value = 0\n",
    "        \n",
    "        for n in self.inbound_nodes:\n",
    "            self.value += n.value\n",
    "            \n",
    "    def backward(self):\n",
    "        self.gradients = {n:np.zeros_like(n.value,dtype='float32') for n in self.inbound_nodes}\n",
    "        \n",
    "        for n in self.outbound_nodes:\n",
    "            grad_cost = n.gradients[self]\n",
    "            print(grad_cost)\n",
    "            \n",
    "            for i in self.inbound_nodes:\n",
    "                self.gradients[i] += np.sum(grad_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.]\n",
      " [ 3.]]\n",
      "{<__main__.Input object at 0x0000021AC7514828>: array([[-3.],\n",
      "       [-3.]]), <__main__.Add object at 0x0000021AC7514DA0>: array([[ 3.],\n",
      "       [ 3.]])}\n",
      "{<__main__.Input object at 0x0000021AC7514860>: array([ 6.,  6.], dtype=float32), <__main__.Input object at 0x0000021AC7514E48>: array([ 6.,  6.], dtype=float32), <__main__.Input object at 0x0000021AC75149B0>: array([ 6.,  6.], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "x,y,z = Input(), Input(),Input()\n",
    "yy = Input()\n",
    "\n",
    "f = Add(x, y, z)\n",
    "\n",
    "x_ = np.array([2, 2])\n",
    "y_ = np.array([3, 3])\n",
    "z_ = np.array([5, 5])\n",
    "yy_ = np.array([7, 7])\n",
    "\n",
    "feed_dict = {x:x_, y:y_, z:z_,yy:yy_}\n",
    "cost = MSE(yy, f)\n",
    "\n",
    "sorted_nodes = topological_sort(feed_dict)\n",
    "\n",
    "# 未进行forward_pass，这个网络没有“跑”，只有输入节点有值\n",
    "#print(f.value, x.value, y.value, z.value)\n",
    "\n",
    "# 让网络向前运行，\n",
    "forward_and_backward(sorted_nodes)\n",
    "print(cost.gradients)\n",
    "print(f.gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "向后传播也写完了，我们可以测试一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ -3.34017280e-05,  -5.01025919e-05],\n",
      "       [ -6.68040138e-05,  -1.00206021e-04]]), array([[ 0.9999833],\n",
      "       [ 1.9999833]]), array([[  5.01028709e-05],\n",
      "       [  1.00205742e-04]]), array([ -5.01028709e-05])]\n"
     ]
    }
   ],
   "source": [
    "X, W, b = Input(), Input(), Input()\n",
    "y = Input()\n",
    "f = Linear(X, W, b)\n",
    "a = Sigmoid(f)\n",
    "cost = MSE(y, a)\n",
    "\n",
    "X_ = np.array([[-1., -2.], [-1, -2]])\n",
    "W_ = np.array([[2.], [3.]])\n",
    "b_ = np.array([-3.])\n",
    "y_ = np.array([1, 2])\n",
    "\n",
    "feed_dict = {\n",
    "    X: X_,\n",
    "    y: y_,\n",
    "    W: W_,\n",
    "    b: b_,\n",
    "}\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "forward_and_backward(graph)\n",
    "# return the gradients for each Input\n",
    "gradients = [t.gradients[t] for t in [X, y, W, b]]\n",
    "\"\"\"\n",
    "Expected output\n",
    "\n",
    "[array([[ -3.34017280e-05,  -5.01025919e-05],\n",
    "       [ -6.68040138e-05,  -1.00206021e-04]]), array([[ 0.9999833],\n",
    "       [ 1.9999833]]), array([[  5.01028709e-05],\n",
    "       [  1.00205742e-04]]), array([ -5.01028709e-05])]\n",
    "\"\"\"\n",
    "print(gradients)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
